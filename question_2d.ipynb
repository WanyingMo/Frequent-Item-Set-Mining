{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4cf9a54-6d4b-46bc-ab91-2fedfd9097e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de47e0be-a43e-43a6-a27e-c4ef88150971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('DAI93865', 'FRO40251'), 1.0), (('GRO85051', 'FRO40251'), 0.999176276771005), (('GRO38636', 'FRO40251'), 0.9906542056074766), (('ELE12951', 'FRO40251'), 0.9905660377358491), (('DAI88079', 'FRO40251'), 0.9867256637168141)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import itemgetter\n",
    "conf = SparkConf()\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "lines = sc.textFile(\"browsing.txt\")\n",
    "\n",
    "products = lines.flatMap(lambda x: x.split(\" \")).countByValue()\n",
    "pro_100={}\n",
    "for key,value in products.items():#single element which appears more than 100\n",
    "    if value>=100:\n",
    "         pro_100.setdefault(str(key),int(value))\n",
    "            \n",
    "def split_line(line):#split every line in to separate elements\n",
    "    pros=[]\n",
    "    for pro in line.split():\n",
    "        pros.append(str(pro))\n",
    "    return pros\n",
    "def pair_product(p):\n",
    "    pairs=[]\n",
    "    n=len(p)\n",
    "    for i in range(0,n-1):\n",
    "        for j in range(i+1,n):\n",
    "            if (p[i] in pro_100) and (p[j] in pro_100):\n",
    "                pairs.append(((p[i],p[j]),1))\n",
    "                pairs.append(((p[j],p[i]),1))\n",
    "    return pairs\n",
    "def confidence(p):\n",
    "    sup=int(pro_100[p[0][0]])\n",
    "    conf=p[1]/sup\n",
    "    return (p[0],conf)\n",
    "pro=lines.map(split_line)\n",
    "p_pairs=pro.flatMap(pair_product)\n",
    "counted=p_pairs.groupByKey() \\\n",
    "    .map(lambda x: (x[0], sum(x[1])))\\\n",
    "    .filter(lambda x: x[1] >= 100)\n",
    "result=counted.map(confidence)\n",
    "result=result.collect()\n",
    "result=sorted(result, key=itemgetter(1,0),reverse=True)\n",
    "print(result[0:5])\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2086d163-21b3-4ab7-8237-0c15075c1f39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62c6be-13ae-4d7a-acc2-297ffac0a0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "* Spark 3 in Python 3.8",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
